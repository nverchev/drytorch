{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b46cb6",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "! uv pip install drytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e248bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchmetrics\n",
    "\n",
    "from torcheval import metrics as eval_metrics\n",
    "\n",
    "from drytorch.core import protocols as p\n",
    "\n",
    "\n",
    "tensor_a = torch.ones(1, 1, dtype=torch.float)\n",
    "tensor_b = 3 * torch.ones(1, 1, dtype=torch.float)\n",
    "torch_metric = torchmetrics.MeanSquaredError()\n",
    "eval_metric = eval_metrics.MeanSquaredError()\n",
    "\n",
    "\n",
    "def is_valid_objective(\n",
    "    metric: p.ObjectiveProtocol[torch.Tensor, torch.Tensor],\n",
    ") -> bool:\n",
    "    \"\"\"Test metric follows the Objective protocol.\"\"\"\n",
    "    return isinstance(metric, p.ObjectiveProtocol)\n",
    "\n",
    "\n",
    "torch_metric.update(tensor_a, tensor_b)\n",
    "eval_metric.update(tensor_a, tensor_b)\n",
    "\n",
    "if not torch.isclose(torch_metric.compute(), eval_metric.compute()):\n",
    "    raise AssertionError('Metrics values should match.')\n",
    "\n",
    "if not (is_valid_objective(eval_metric) and is_valid_objective(torch_metric)):\n",
    "    raise AssertionError('These objects should follow the ObjectiveProtocol.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "807a6706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_loss(\n",
    "    metric: p.LossProtocol[torch.Tensor, torch.Tensor],\n",
    ") -> bool:\n",
    "    \"\"\"Test metric follows the Loss protocol.\"\"\"\n",
    "    return isinstance(metric, p.LossProtocol)\n",
    "\n",
    "\n",
    "if not is_valid_loss(torch_metric):\n",
    "    raise AssertionError('This object should also follow the LossProtocol.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32198a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from drytorch.contrib.torchmetrics import from_torchmetrics\n",
    "\n",
    "\n",
    "new_metric = 1 + torch_metric\n",
    "imported_metric = from_torchmetrics(new_metric)\n",
    "imported_metric.update(tensor_a, tensor_b)\n",
    "expected_metrics_from_torchmetrics = {\n",
    "    'Combined Loss': torch.tensor(5.0),\n",
    "    'MeanSquaredError': torch.tensor(4.0),\n",
    "}\n",
    "if not imported_metric.compute() == expected_metrics_from_torchmetrics:\n",
    "    raise AssertionError('Metrics values should be as expected.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab908171",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import mse_loss as mse_loss_fn  # returns scalar value\n",
    "\n",
    "from drytorch.lib.objectives import Metric\n",
    "\n",
    "\n",
    "def mae_loss_fn(outputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Returns batched Meas Absolute Error (MAE) values.\"\"\"\n",
    "    return torch.abs(outputs - targets).flatten(1).mean(1)\n",
    "\n",
    "\n",
    "mse_metric = Metric(mse_loss_fn, name='MSE', higher_is_better=False)\n",
    "mae_metric = Metric(mae_loss_fn, 'MAE', higher_is_better=False)\n",
    "metric_collection = mse_metric | mae_metric\n",
    "metric_collection.update(tensor_a, tensor_b)\n",
    "metric_collection.compute()\n",
    "expected_metric_collection = {\n",
    "    'MSE': torch.tensor(4.0),\n",
    "    'MAE': torch.tensor(2.0),\n",
    "}\n",
    "if not metric_collection.compute() == expected_metric_collection:\n",
    "    raise AssertionError('Metrics values should be as expected.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1eb12f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import override\n",
    "\n",
    "from drytorch.lib.objectives import Objective\n",
    "\n",
    "\n",
    "class MyMetrics(Objective[torch.Tensor, torch.Tensor]):\n",
    "    \"\"\"Class to calculate MSE and MAE more efficiently.\"\"\"\n",
    "\n",
    "    @override\n",
    "    def calculate(\n",
    "        self, outputs: torch.Tensor, targets: torch.Tensor\n",
    "    ) -> dict[str, torch.Tensor]:\n",
    "        diff = outputs - targets\n",
    "        return {\n",
    "            'MSE': torch.pow(diff, 2).flatten(1).mean(1),\n",
    "            'MAE': torch.abs(diff).flatten(1).mean(1),\n",
    "        }\n",
    "\n",
    "\n",
    "my_metrics = MyMetrics()\n",
    "my_metrics.update(tensor_a, tensor_b)\n",
    "my_metrics.compute()\n",
    "if not my_metrics.compute() == expected_metric_collection:\n",
    "    raise AssertionError('Metrics values should be as before.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54f0b316",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import mse_loss as mse_loss_fn  # returns scalar value\n",
    "\n",
    "from drytorch.lib.objectives import Loss\n",
    "\n",
    "\n",
    "mse_loss = Loss(mse_loss_fn, name='MSE')\n",
    "mae_loss = Loss(mae_loss_fn, 'MAE')\n",
    "composed_loss = mse_loss**2 + 0.5 * mae_loss\n",
    "composed_loss.update(tensor_a, tensor_b)\n",
    "expected_metrics_from_loss = {\n",
    "    'Combined Loss': torch.tensor(17.0),\n",
    "    'MSE': torch.tensor(4.0),\n",
    "    'MAE': torch.tensor(2.0),\n",
    "}\n",
    "if not composed_loss.compute() == expected_metrics_from_loss:\n",
    "    raise AssertionError('Metrics values should be as expected.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bd9ee6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if composed_loss.formula != '[MSE]^2 + 0.5 x [MAE]':\n",
    "    raise AssertionError('Formula mismatch.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a49eafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "World size is 1, and metric(s) not synced. returning the input metric(s).\n"
     ]
    }
   ],
   "source": [
    "from drytorch.contrib.torcheval import from_torcheval\n",
    "\n",
    "\n",
    "eval_metric_with_sync = from_torcheval(eval_metric)\n",
    "\n",
    "eval_metric_with_sync.sync()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.18.1"
   }
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "source_map": [
   13,
   37,
   41,
   64,
   96,
   108,
   121,
   135,
   157,
   163,
   188,
   202,
   221,
   224,
   236
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}