{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Metrics and Losses\n",
    "\n",
    "DRYTorch helps you **standardize and document** your model's metrics and loss.\n",
    "\n",
    "### Terminology\n",
    "\n",
    "An **objective** is a criterion for model performance evaluation. We distinguish between two types:\n",
    "\n",
    "* **Metric:** Assesses model performance as a proxy for the overall goal.\n",
    "* **Loss:** Optimizes the model parameters to improve metric assessments.\n",
    "\n",
    "DRYTorch allows, and often **implements** by default, using losses as metrics.\n",
    "\n",
    "### Protocols\n",
    "\n",
    "DRYTorch defines an `ObjectiveProtocol`, used by classes that implement the validation and testing of a model, and a `LossProtocol`, used for its training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Compatibility with Existing Libraries\n",
    "\n",
    "DRYTorch **does not re-implement** common metrics or losses. Instead, it defines **protocols** to ensure **full compatibility** with classes from popular existing libraries.\n",
    "\n",
    "\n",
    "### For Validation and Testing\n",
    "\n",
    "The `ObjectiveProtocol` is compatible with `Metric` classes from:\n",
    "* [**TorchMetrics**](https://lightning.ai/docs/torchmetrics/stable/)\n",
    "* [**TorchEval**](https://docs.pytorch.org/torcheval/stable/)\n",
    "\n",
    "You can use instances of these third-party metric classes **directly** when defining a DRYTorch validation or test step.\n",
    "\n",
    "### For Training\n",
    "\n",
    "The `LossProtocol` is designed to accept any class that meets its requirements, including some metrics. You can therefore use differentiable metrics from libraries like **TorchMetrics** directly when building a DRYTorch training class.\n",
    "\n",
    "\n",
    "**TorchMetrics** also offers a `CompositionalMetric`, with support for algebra operations, which inspired part of the DRYTorch own implementation. To make it compatible with the framework and better documentation, you can use\n",
    "`from_torchmetrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchmetrics\n",
    "\n",
    "from torcheval import metrics as eval_metrics\n",
    "\n",
    "from drytorch.core import protocols as p\n",
    "\n",
    "\n",
    "torch_metric = torchmetrics.MeanSquaredError()\n",
    "eval_metric = eval_metrics.MeanSquaredError()\n",
    "\n",
    "\n",
    "def supported_for_validation(\n",
    "    metric: p.ObjectiveProtocol[torch.Tensor, torch.Tensor],\n",
    ") -> bool:\n",
    "    \"\"\"Test metric follows the Objective protocol.\"\"\"\n",
    "    return isinstance(metric, p.ObjectiveProtocol)\n",
    "\n",
    "\n",
    "supported_for_validation(eval_metric) and supported_for_validation(torch_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def supported_for_training(\n",
    "    metric: p.LossProtocol[torch.Tensor, torch.Tensor],\n",
    ") -> bool:\n",
    "    \"\"\"Test metric follows the Loss protocol.\"\"\"\n",
    "    return isinstance(metric, p.LossProtocol)\n",
    "\n",
    "\n",
    "supported_for_training(torch_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from drytorch.contrib.torchmetrics import from_torchmetrics\n",
    "\n",
    "\n",
    "new_metric = torch_metric + torch_metric\n",
    "\n",
    "\n",
    "imported_metric = from_torchmetrics(new_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## DRYTorch implementation\n",
    "DRYTorch objective classes act as wrappers around **user-defined** metric and loss callables.\n",
    "\n",
    "\n",
    "These callables must accept model outputs and targets as arguments and return a scalar PyTorch Tensor for an aggregated mini-batch value or a vector of batched values (recommended for more precise averaging across batches of varying sizes). The abstract `Objective` class handles **calling the logic, documenting, and correctly aggregating** the results across batches.\n",
    "\n",
    "\n",
    "### The Metric and MetricCollection classes\n",
    "The `Metric` class is to define a single metric. You can document it by giving it an explicit name and specify whether it is better when higher or lower. You can also concatenate different Metric instances with compatible signatures into a `MetricCollection` instance, or creating one directly from a dictionary of named metric functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import mse_loss as mse_loss_fn  # returns scalar value\n",
    "\n",
    "from drytorch.lib.objectives import Metric\n",
    "\n",
    "\n",
    "def mae_loss_fn(outputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Returns batched Meas Absolute Error (MAE) values.\"\"\"\n",
    "    return torch.abs(outputs - targets).flatten().mean(1)\n",
    "\n",
    "\n",
    "mse_metric = Metric(mse_loss_fn, name='MSE', higher_is_better=False)\n",
    "mae_metric = Metric(mae_loss_fn, 'MAE', higher_is_better=False)\n",
    "metric_collection = mse_metric | mae_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Define a Custom Metric class\n",
    "\n",
    "You can subclass the abstract `Objective` class by overriding the `calculate` method. In this example, we slightly reduce the calculation overhead to obtain the previous metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import override\n",
    "\n",
    "from drytorch.lib.objectives import Objective\n",
    "\n",
    "\n",
    "class MyMetrics(Objective[torch.Tensor, torch.Tensor]):\n",
    "    \"\"\"Class to calculate MSE and MAE more efficiently.\"\"\"\n",
    "\n",
    "    @override\n",
    "    def calculate(\n",
    "        self, outputs: torch.Tensor, targets: torch.Tensor\n",
    "    ) -> dict[str, torch.Tensor]:\n",
    "        diff = outputs - targets\n",
    "        return {\n",
    "            'MSE': torch.pow(diff, 2).flatten().mean(1),\n",
    "            'MAE': torch.abs(diff).flatten().mean(1),\n",
    "        }\n",
    "\n",
    "\n",
    "my_metrics = MyMetrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## LossBase, Loss and CompositionalLoss\n",
    "\n",
    "`LossBase` is the abstract class for concrete loss classes, such as `Loss` and `CompositionalLoss`.\n",
    "\n",
    "`Loss` is equivalent to Metric and accepts a single callable that is used both as a criterion for backpropagation for the loss and as a metric.\n",
    "\n",
    "\n",
    "The `CompositionalLoss` class extends this idea by evaluating other metrics besides the main optimization criterion. This allows you to easily document and track the performance of the **single components** that make up a more complex, composed loss function.\n",
    "\n",
    "\n",
    "It is possible to create a compositional loss by using simple algebraic operations between a `LossBase` instance and an integer, float, or another `LossBase` instance. The resulting object's `formula` attribute documents the specific operations and component losses utilized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import mse_loss as mse_loss_fn  # returns scalar value\n",
    "\n",
    "from drytorch.lib.objectives import Loss\n",
    "\n",
    "\n",
    "def mae_loss_fn(outputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Returns batched Meas Absolute Error (MAE) values.\"\"\"\n",
    "    return torch.abs(outputs - targets).flatten().mean(1)\n",
    "\n",
    "\n",
    "mse_loss = Loss(mse_loss_fn, name='MSE')\n",
    "mae_loss = Loss(mae_loss_fn, 'MAE')\n",
    "composed_loss = mse_loss**2 + 0.5 * mae_loss\n",
    "\n",
    "composed_loss.formula"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "default_lexer": "ipython3",
   "formats": "ipynb,md:myst"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
